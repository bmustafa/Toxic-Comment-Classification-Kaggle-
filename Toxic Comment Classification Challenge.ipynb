{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport nltk\nnltk.download('stopwords')\nimport re\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nMAX_FEATURES = 20000\nMAX_WORD = 100\nEMBEDDING_SIZE = 300\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest_df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\nsample_submission = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_prepare(text):\n    \"\"\"Tokenization and Preprocessing.\"\"\"\n    \n    # Make everything lowercase\n    text = text.lower()\n    \n    # Remove misspelled words or words not found in GoogleNews embeddings (determined in data exploration and preprocessing)\n    text = re.sub(\"doesnt\", \"does not\", text)\n    text = re.sub(\"dont\", \"do not\", text)\n    text = re.sub(\"isnt\", \"is not\", text)\n    text = re.sub(\"wasnt\", \"was not\", text)\n    text = re.sub(\"didnt\", \"did not\", text)\n    text = re.sub(\"behaviour\", \"behavior\", text)\n    text = re.sub(\"colour\", \"color\", text)\n    \n    # Replace symbols,newline characters and remove stopwords. Then tokenize sentence \n    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;]')\n    good_symbols_re = re.compile('[^0-9a-z #+_]')\n    stopwords_set = set(stopwords.words('english'))\n    text = re.sub(\"\\n\", \" \", text)\n    text = replace_by_space_re.sub(' ', text)\n    text = good_symbols_re.sub('', text)\n    text = ' '.join([x for x in nltk_tokenize(text) if x and x not in stopwords_set])\n    \n    return text.strip()\n\n\ndef nltk_tokenize(text):\n    \"\"\"Used to split a sentence into tokens\"\"\"\n    \n    tokens = word_tokenize(text)\n    return tokens\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean comments in our training set and test set\ntrain_cleaned_x = train_x['comment_text'].map(lambda x: text_prepare(x))\ntest_cleaned_x = test_x['comment_text'].map(lambda x: text_prepare(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Use keras tokenizer to find 20,000 most important words and prepare for them to be fed into embedding layer\ntokenizer = Tokenizer(MAX_FEATURES)\ntokenizer.fit_on_texts(list(train_cleaned_x))\ntrain_tokenized_x = tokenizer.texts_to_sequences(train_cleaned_x)\ntest_tokenized_x = tokenizer.texts_to_sequences(test_cleaned_x)\n\n# Pad input so that they are all of the same length\npadded_train_x = pad_sequences(train_tokenized_x, maxlen = MAX_WORD)\npadded_test_x = pad_sequences(test_tokenized_x, MAX_WORD)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nIGNORE: USED IN DATA EXPLORATION\n\n\ndef create_dict(sentences):\n    dict = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                dict[word] += 1\n            except:\n                dict[word] = 1\n    return dict\n\ndef unk(dict, word2vec):\n    unk_token = {}\n    for word in dict:\n        try:\n            word2vec[word]\n        except:\n            unk_token[word] = dict[word]\n    return unk_token\n\"\"\"      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\n\n# Use gensim to load GoogleNews pre-trained word embeddings\nINPUT_FILE = \"../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin\"\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(INPUT_FILE, binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert gensim object into a python dictionary\ndict_size = len(tokenizer.word_index)\nembedding_index = {}\nfor word in word2vec.wv.vocab:\n    embedding_index[word] = word2vec.word_vec(word)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a matrix of embeddings and pick the default values from the distribution of matrix weights\nembeddings = np.stack(list(embedding_index.values()))\nembeddings_mean = embeddings.mean()\nembeddings_std_dev = embeddings.std()\nembedding_matrix = np.random.normal(embeddings_mean, embeddings_std_dev, (len(tokenizer.word_index), EMBEDDING_SIZE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For words from the source text (fed into the tokenizer), change the weights in the matrix to match their embedding values\nfor word, index in tokenizer.word_index.items():\n    if word in embedding_index:\n        embedding_matrix[index - 1] = embedding_index[word]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import model and layers used\nfrom keras.models import Model\nfrom keras.layers import  Embedding, Input, concatenate, SpatialDropout1D, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, Bidirectional, GRU, Dense\n\n# Use keras to define model\nemb = Input(shape=(None,))\nx = Embedding(dict_size,embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(emb)\nx = SpatialDropout1D(0.2)(x)\nx = Bidirectional(GRU(256, return_sequences = True, dropout = 0.2, recurrent_dropout = 0.2))(x)\nx = Conv1D(64, kernel_size = 4)(x)\nx = concatenate([GlobalMaxPooling1D()(x), GlobalAveragePooling1D()(x)])\nx = Dense(6, activation = \"sigmoid\")(x)\nmodel = Model(emb, x)\nmodel.compile(loss = 'binary_crossentropy', optimizer= 'adam', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit model on our padded data\nclassifier = model.fit(padded_train_x, train_y.values, batch_size = 1024, epochs = 5, validation_split = 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict outcomes for our test data and save as a csv\nprediction = model.predict(padded_test_x, batch_size = 1024)\nsample_submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = prediction\nsample_submission.to_csv('submission.csv', index = False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}